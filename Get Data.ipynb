{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ceb338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_last_date_from_csv(csv_file):\n",
    "    \"\"\"Get the last date from existing CSV file using system tail command\"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Use tail to get the last line\n",
    "        result = subprocess.run(['tail', '-n', '1', csv_file], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        last_line = result.stdout.strip()\n",
    "        \n",
    "        if not last_line:\n",
    "            return None\n",
    "        \n",
    "        # Extract the first column (create_ts) from the last line\n",
    "        first_column = last_line.split(',')[0]\n",
    "        \n",
    "        # Parse the timestamp and extract date\n",
    "        last_datetime = pd.to_datetime(first_column)\n",
    "        last_date = last_datetime.date()\n",
    "        next_date = last_date + timedelta(days=1)\n",
    "        \n",
    "        print(f\"Found existing data. Last date: {last_date}, resuming from: {next_date}\")\n",
    "        return next_date\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading last date with tail: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_kalshi_data(csv_file='kalshi_trade_data_2025.csv'):\n",
    "    \"\"\"Download Kalshi trade data with auto-resume using tail command\"\"\"\n",
    "    \n",
    "    # Determine start date\n",
    "    start_date = get_last_date_from_csv(csv_file)\n",
    "    if start_date is None:\n",
    "        start_date = datetime(2025, 1, 1).date()\n",
    "        print(\"Starting fresh download from 2025-01-01\")\n",
    "        file_mode = 'w'\n",
    "        write_header = True\n",
    "    else:\n",
    "        file_mode = 'a'\n",
    "        write_header = False\n",
    "    \n",
    "    # End date is yesterday\n",
    "    end_date = (datetime.now() - timedelta(days=1)).date()\n",
    "    \n",
    "    if start_date > end_date:\n",
    "        print(\"Already up to date!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloading from {start_date} to {end_date}\")\n",
    "    \n",
    "    base_url = 'https://kalshi-public-docs.s3.amazonaws.com/reporting/trade_data_{}.json'\n",
    "    \n",
    "    # Generate date range\n",
    "    current_date = start_date\n",
    "    dates_to_process = []\n",
    "    while current_date <= end_date:\n",
    "        dates_to_process.append(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    successful_count = 0\n",
    "    failed_dates = []\n",
    "    batch_data = []\n",
    "    batch_size = 5  # Save every 5 successful downloads\n",
    "    \n",
    "    for date_str in tqdm(dates_to_process, desc=\"Downloading trade data\"):\n",
    "        url = base_url.format(date_str)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data:  # Only process if data exists\n",
    "                    df = pd.DataFrame(data)\n",
    "                    \n",
    "                    # Convert create_ts to timestamp\n",
    "                    if 'create_ts' in df.columns:\n",
    "                        df['create_ts'] = pd.to_datetime(df['create_ts'])\n",
    "                    \n",
    "                    # Reorder columns: create_ts, ticker_name, contracts_traded, price\n",
    "                    desired_columns = ['create_ts', 'ticker_name', 'contracts_traded', 'price']\n",
    "                    available_columns = [col for col in desired_columns if col in df.columns]\n",
    "                    \n",
    "                    if available_columns:\n",
    "                        df = df[available_columns]\n",
    "                        batch_data.append(df)\n",
    "                        successful_count += 1\n",
    "                        \n",
    "                        # Save in batches\n",
    "                        if len(batch_data) >= batch_size:\n",
    "                            save_batch_to_csv(batch_data, csv_file, write_header and successful_count == len(batch_data))\n",
    "                            batch_data = []\n",
    "                            write_header = False\n",
    "                    else:\n",
    "                        failed_dates.append(date_str)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            failed_dates.append(date_str)\n",
    "        \n",
    "        time.sleep(0.05)  # Be nice to the server\n",
    "    \n",
    "    # Save any remaining batch data\n",
    "    if batch_data:\n",
    "        save_batch_to_csv(batch_data, csv_file, write_header and successful_count == len(batch_data))\n",
    "    \n",
    "    print(f\"\\nDownload complete!\")\n",
    "    print(f\"Successfully processed: {successful_count} days\")\n",
    "    if failed_dates:\n",
    "        print(f\"Failed downloads: {len(failed_dates)} days\")\n",
    "\n",
    "def save_batch_to_csv(batch_data, csv_file, write_header):\n",
    "    \"\"\"Save a batch of dataframes to CSV\"\"\"\n",
    "    if not batch_data:\n",
    "        return\n",
    "        \n",
    "    combined_batch = pd.concat(batch_data, ignore_index=True)\n",
    "    \n",
    "    combined_batch.to_csv(\n",
    "        csv_file, \n",
    "        mode='a' if not write_header else 'w',\n",
    "        header=write_header,\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "def get_file_stats(csv_file='data/kalshi_trade_data_2025.csv'):\n",
    "    \"\"\"Get file statistics using system commands\"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"File {csv_file} not found\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Get line count (subtract 1 for header)\n",
    "        result = subprocess.run(['wc', '-l', csv_file], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        line_count = int(result.stdout.split()[0]) - 1\n",
    "        \n",
    "        # Get first few lines to show sample\n",
    "        result = subprocess.run(['head', '-n', '6', csv_file], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        \n",
    "        print(f\"\\nFile: {csv_file}\")\n",
    "        print(f\"Total records: {line_count:,}\")\n",
    "        print(f\"\\nSample data:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file stats: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e9bc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh download from 2025-01-01\n",
      "Downloading from 2025-01-01 to 2025-08-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading trade data:  30%|██▉       | 67/224 [02:52<12:43,  4.86s/it]/var/folders/09/fwj9ml5j17z0dlwbz1b4vwph0000gn/T/ipykernel_29579/818351770.py:87: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df['create_ts'] = pd.to_datetime(df['create_ts'])\n",
      "Downloading trade data: 100%|██████████| 224/224 [11:36<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete!\n",
      "Successfully processed: 217 days\n"
     ]
    }
   ],
   "source": [
    "download_kalshi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062312e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
